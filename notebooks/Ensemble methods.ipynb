{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll take a look on how to implement three different ensemble learning algorithms, using some functionatilies from numpy and sklearn. The algorithms are: AdaBoost, Gradient Boosting and Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaBoost is an ensemble model for supervised learning first described by Yoav Freund and Robert Schapire. The algorithm consists of a boosting technique, that takes a weak classifier,e.g., decision bump, and improves its performance by applying weights to each sucessive prediction on each iteration. At the end of the iteration loop, the algorithm makes a weighted \"vote\" on the final prediction. This implementation of the AdaBoost only works for classification problems with $y \\in \\{-1,1\\} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.tree  import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification(y,prediction):\n",
    "    \"\"\"\n",
    "    Calculates the number of misclassifications between the prediction and the output.\n",
    "    Inputs:\n",
    "        y: int\n",
    "            Input a single value of the output variable y.\n",
    "        prediction: int\n",
    "            Input a single valut of the predicted output.\n",
    "    Returns:\n",
    "        misclassifications: array\n",
    "            Output 1 if the values do not match and 0 if they do.\n",
    "    \"\"\"\n",
    "    y=y.reshape((-1,1))\n",
    "    prediction = prediction.reshape((-1,1))\n",
    "    misclassifications = 1*(y != prediction)\n",
    "    return misclassifications #returns the number of misclassifications on the prediction\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    \"\"\"\n",
    "    AdaBoost algorithm for weak classifiers, that can fit discrete classification problems.\n",
    "    Methods:\n",
    "        fit(x,y) -> Performs the boosting algorithm on the training set(x,y).\n",
    "        predict(x) -> Predict class for X.\n",
    "        get_tree_weights() -> Returns the weights for each of the n_iter trees generated during the boosting task.\n",
    "    \"\"\"\n",
    "    def __init__(self,n_estimators):\n",
    "        \"\"\"\n",
    "        Initialize self.\n",
    "        Inputs:\n",
    "            n_estimators: int\n",
    "                input the number of trees(stumps) to grow.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        Fits the AdaBooster on a given dataset.\n",
    "        Inputs:\n",
    "            X: array\n",
    "                input the array of input points\n",
    "            y: array\n",
    "                input the array of output points, with y E {-1,1}   \n",
    "        \"\"\"\n",
    "        self.input_train = X \n",
    "        self.output_train = y \n",
    "        alphas = np.zeros((self.n_estimators,1)) \n",
    "        predictions_train = np.zeros((len(self.output_train),self.n_estimators)) \n",
    "        staged_weights = np.zeros((len(self.output_train),self.n_estimators)) \n",
    "        weighted_error = np.zeros((self.n_estimators,1)) \n",
    "        stumps = list()\n",
    "\n",
    "        for i in range(len(self.output_train)):\n",
    "            staged_weights[i,0] = 1/len(self.output_train) #initialized the weights with value 1/num_samples\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "\n",
    "            curr_weights = staged_weights[:,m] #current staged weights\n",
    "            stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n",
    "            stump = stump.fit(self.input_train,self.output_train,sample_weight=curr_weights) # fits a decision stump with curr_weights as the sample weight\n",
    "\n",
    "            predictions_train[:,m] = stump.predict(self.input_train) # stores the current predictions\n",
    "            curr_weights = curr_weights.reshape((-1,1)) \n",
    "            weighted_misclassification = curr_weights * misclassification(self.output_train,predictions_train[:,m]) #sets the weighted misclassification as the product between the current weights and the misclassification between the current prediction and the original output\n",
    "            weighted_error[m] = np.sum(weighted_misclassification)/np.sum(curr_weights) #calculates the weighted error\n",
    "            alphas[m] = np.log((1 - weighted_error[m])/weighted_error[m]) # calculates the constants alpha\n",
    "            curr_weights = curr_weights * np.exp(alphas[m] * misclassification(self.output_train,predictions_train[:,m])) #sets the new weights\n",
    "            curr_weights = curr_weights.reshape((-1,)) # reshapes it to be 1D\n",
    "\n",
    "            if m + 1 < self.n_estimators: # if this iteration isn't the last one, sets the weights for the next iteration\n",
    "                staged_weights[:,m+1] = curr_weights\n",
    "            stumps.append(stump)    \n",
    "\n",
    "        self.tree_weights = alphas\n",
    "        self.stumps_list = stumps\n",
    "        self.weighted_error = weighted_error\n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the weights and classifiers calculated by the Adabooster\n",
    "        Inputs:\n",
    "            x: array_like\n",
    "                input the array of input points\n",
    "        Returns:\n",
    "            self.predict: array_like\n",
    "                outputs the array of predictions\n",
    "        \"\"\"   \n",
    "        indiv_predictions = np.array([self.classifier.predict(x) for self.classifier in self.stumps_list])\n",
    "        prediction = indiv_predictions.T @ self.tree_weights   #multiplies each tree prediction by the trees calculated weight      \n",
    "        prediction = np.sign(prediction) #the adaboost final prediction will be the sign of the previous operation.\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def get_tree_weights(self):\n",
    "        \"\"\"\n",
    "        Gives the weights calculated by the AdaBooster\n",
    "        \"\"\"\n",
    "        return self.tree_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting is an ensemble model for supervised learning first described by Jerome H. Friedman. It consists of a boosting model, that iteratively calculates the generalized(or partial) residuals at each iteration M, and then fits a new decision tree(regression or classification) targeting these residuals. The algorithm also finds the constant values(gamma) for each split on the decision tree that minimizes the inserted loss function. This model differs from AdaBoost in the sense that it allows for any type of loss function to be used, so long as the function is differentiable. This means that the Gradient Boosting algorithm can fit any type of classification or regression task, which is a big improvement with respect to AdaBoost, since it's most basic implementation can only fit classification problems with target values $y \\in \\{-1,1\\} $. There are versions of AdaBoost that allow for regression tasks, but the model itself is based on the aforementioned assumption about the nature of the output set, so it is far more limited than the Gradient Boosting technique. In this example, we'll be implementing a Gradient Boosted Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.tree  import DecisionTreeRegressor\n",
    "from sklearn.tree._tree import TREE_LEAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_func(y,prediction,loss_func):\n",
    "    \"\"\"\n",
    "    Calcultes the generalized(or partial) residuals between the prediction and the output. The residuals are defined to be the negative value of the gradient of the loss function w.r.t. prediction.\n",
    "    Inputs:\n",
    "        y: array\n",
    "            Input the array of outputs.\n",
    "        prediction: array\n",
    "            Input the array of predictions.\n",
    "        loss_func: string\n",
    "            Input the string that identifies the loss function to use when calculating the residuals; loss_func can be 'multinomial deviance'(multinomial deviance loss), 'entropy'(Cross-entropy loss), 'mse'(Mean Squared Error), 'mae'(Mean Absolute error).\n",
    "    \"\"\"\n",
    "    possible_loss_funcs = ['mse','mae']\n",
    "    assert loss_func in possible_loss_funcs\n",
    "\n",
    "    if loss_func == possible_loss_funcs[0]:\n",
    "        return (y - prediction)\n",
    "    else:\n",
    "        return (2*(y - prediction) - 1).reshape((-1,1))\n",
    "\n",
    "def optimal_gamma(y,prediction,loss_func):\n",
    "    \"\"\"\n",
    "    Calculates the optimal value for the gamma constant based on a certain loss_func.\n",
    "    Inputs:\n",
    "        y: array\n",
    "            input the array of outputs/targets.\n",
    "        prediction: array\n",
    "            input the array of predictions.\n",
    "        loss_func: string\n",
    "            input the string that identifies the loss function to be used; loss_func can be: 'mse'(Mean Squared Error) or 'mae'(Mean Absolute Error)\n",
    "    \"\"\"\n",
    "    assert loss_func in [\"mse\",\"mae\"]\n",
    "\n",
    "    if loss_func == 'mae':\n",
    "        res = y - prediction\n",
    "        return np.median(res)  \n",
    "\n",
    "    elif loss_func == 'mse':\n",
    "        res = y - prediction\n",
    "        return np.mean(res)\n",
    "\n",
    "class GBRegressor:\n",
    "    \"\"\"\n",
    "    GradientBoost algorithm for supervised learning, that can fit regression problems.\n",
    "    Methods:\n",
    "        fit(X,y) -> Performs the gradient boosting algorithm on the training set(x,y).\n",
    "        predict(x) -> Predict value for X.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,n_estimators,loss_func,max_depth=None,random_state=None):\n",
    "        \"\"\"\n",
    "        Inilialize self.\n",
    "        Inputs:\n",
    "            n_estimators: int\n",
    "                input the number of trees to grow.\n",
    "            loss_func: string\n",
    "                input the string that identifies the loss function to use when calculating the residuals; loss_func can be  'mse'(Mean Squared Error), 'mae'(Mean Absolute error).\n",
    "            max_depth: int\n",
    "                input the maximum depth of the tree; default is set to None.\n",
    "            random_state: int\n",
    "                input the random_state to be used on the sklearn DecisionTreeClassifier; default is set to None.\n",
    "            \n",
    "                \n",
    "        \"\"\"\n",
    "        possible_params = ['mse', 'mae']\n",
    "        assert n_estimators > 0\n",
    "        assert loss_func in possible_params\n",
    "        if max_depth != None:\n",
    "            assert max_depth >= 1\n",
    "            \n",
    "        self.n_estimators = n_estimators\n",
    "        self.loss_func = loss_func\n",
    "        self.random_state = random_state\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        Fits the GradientBooster model.\n",
    "        Inputs:\n",
    "            X: array\n",
    "                input array of input points.\n",
    "            y: array\n",
    "                input array of output points.\n",
    "\n",
    "        \"\"\"\n",
    "        self.input_train = X\n",
    "        self.output_train = y\n",
    "        self.trained_trees_list = list()\n",
    "        self.gammas = list()\n",
    "        self.gamma_0 = optimal_gamma(self.output_train,np.zeros(self.output_train.shape[0]),self.loss_func) #initializes gamma as the optimal value between the output and a vector of zeroes\n",
    "        raw_pred = np.ones((self.output_train.shape[0])) * self.gamma_0 #makes the initial prediction\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            residuals = residuals_func(self.output_train,raw_pred,self.loss_func) # calculates the residuals between the initial prediction and the output\n",
    "            model = DecisionTreeRegressor(criterion = self.loss_func, random_state = self.random_state,max_depth=self.max_depth) \n",
    "            tree = model.fit(self.input_train,residuals) #fits a tree targeting those residuals\n",
    "            terminal_regions = tree.apply(self.input_train) # gets terminal nodes for the tree\n",
    "            gamma = np.zeros((len(tree.tree_.children_left))) # generates a gamma vector, with size = number of terminal nodes\n",
    "\n",
    "            for leaf in np.where(tree.tree_.children_left == TREE_LEAF)[0]: # searches through the tree for terminal nodes(leafs)\n",
    "                mask = np.where(terminal_regions == leaf) # stores the position of each leaf\n",
    "                gamma[leaf] = optimal_gamma(self.output_train[mask],raw_pred[mask],self.loss_func) #finds the best gamma for each leaf\n",
    "\n",
    "            raw_pred += gamma[terminal_regions] # those gammas are then summed to the initial prediction\n",
    "            self.trained_trees_list.append(tree)\n",
    "            self.gammas.append(gamma)\n",
    "\n",
    "                \n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Predicts the value or class of a given group of inputs points based on the trained trees.\n",
    "        Inputs:\n",
    "            x: array_like\n",
    "                input the input point/array to be predicted.\n",
    "        Returns:\n",
    "            final_prediction: array_like\n",
    "                outputs the class/value prediction of the input made by the gradient booster model.\n",
    "        \"\"\"\n",
    "        raw_pred = np.ones(x.shape[0])* self.gamma_0\n",
    "\n",
    "        for tree,gamma in zip(self.trained_trees_list,self.gammas):\n",
    "            terminal_regions = tree.apply(x)\n",
    "            raw_pred += gamma[terminal_regions] #the final prediction of the gradient boosting regressor will be the initial predictions summed with all the optimal gammas found for all leafs\n",
    "\n",
    "        return raw_pred                        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forests is an ensemble model for supervised learning first described by Breiman (2004). It uses bagging to build a group of subsets of the initial dataset, and then builds a tree(decision or regression) for each subset, by randomly choosing a group of features in each subset, so that n_features_used < n_features_total. Here I implement two different Random Forest models, for classification and regression. These implementations are built on top of the sklearn and numpy libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.tree  import DecisionTreeClassifier\n",
    "from sklearn.tree  import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomForestClassifier:\n",
    "    \"\"\"\n",
    "    Class of the Random Forest Classifier Model.\n",
    "    Methods:\n",
    "        fit(X,y) -> Performs the random forests algorithm on the training set(x,y).\n",
    "        predict(x) -> Predict class for X.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,n_estimators,n_classes,max_depth=None,criterion='gini',random_state=None,max_features='sqrt'):\n",
    "        \"\"\"\n",
    "        Initialize self.\n",
    "        Inputs:\n",
    "            n_estimators: int\n",
    "                input the number of trees to grow.\n",
    "            n_classes: int\n",
    "                input the number of classes of the classification task.\n",
    "            max_depth: int\n",
    "                input the maximum depth of the tree; default is set to None.\n",
    "            criterion: string\n",
    "                input string that identifies the criterion to be used when deciding how to split each node. criterion can be: 'gini' or 'entropy' .default is set to 'gini'.\n",
    "            max_features = string or int/float:\n",
    "                input string or int/float that identifies the maximum number of features to be used when splitting each decision tree; if string can be: 'sqrt' or 'log2'; if int max_fetures will be the maximum number of features; if float the maximum number of features will be int(max_features * n_features).                   \n",
    "            random_state: int\n",
    "                input the random_state to be used on the sklearn DecisionTreeClassifier. default is set to None.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        self.n_classes = n_classes\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        possible_criterion = ['gini','entropy']\n",
    "        assert self.criterion in possible_criterion \n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        Fits the RandomForestClassifier model.\n",
    "        Inputs:\n",
    "            X: array\n",
    "                input array of input points.\n",
    "            y: array\n",
    "                input array of output points.\n",
    "        \"\"\"\n",
    "        self.input_train = X\n",
    "        self.output_train = y.reshape((-1,1))\n",
    "        self.trained_trees_list = list()\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            train_inds = np.random.choice(int(self.input_train.shape[0]/2),int(self.input_train.shape[0]/2),False) #generates the indices for a bootstrap sample with size N/2 and with values that don't repeat(doing this improves the efficiency of the algorithm and does not deprecate performance)\n",
    "            model_tree = DecisionTreeClassifier(criterion = self.criterion, random_state = self.random_state,max_features = self.max_features,max_depth=self.max_depth)\n",
    "            model_tree = model_tree.fit(self.input_train[train_inds,:],self.output_train[train_inds,:]) #fits a decision tree with the bootstrap sample\n",
    "            self.trained_trees_list.append(model_tree)\n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Predicts the class of a given group of inputs points based on the trained trees. \n",
    "        Inputs:\n",
    "            x: array_like\n",
    "                input the input point/array to be predicted.\n",
    "        Returns:\n",
    "            final_prediction: array_like\n",
    "                outputs the class prediction of the input made by the random forest model.\n",
    "        \"\"\"\n",
    "        indiv_predictions = np.array([self.classifier.predict(x) for self.classifier in self.trained_trees_list]).T\n",
    "        final_prediction = np.zeros((indiv_predictions.shape[0],))\n",
    "        counter_vec = np.zeros((self.n_classes,1))\n",
    "\n",
    "        for i in range(indiv_predictions.shape[0]):\n",
    "            for j in range(indiv_predictions.shape[1]):\n",
    "                counter_vec[indiv_predictions[i][j]] += 1\n",
    "                final_prediction[i] = np.argmax(counter_vec) #the final prediction of the random forest classifier will be a majority vote made w.r.t. to all trees,i.e., for each output point, predict the class that appears the most on the group of all prediction made by the forest\n",
    "            counter_vec = np.zeros((counter_vec.shape))\n",
    "\n",
    "        return final_prediction\n",
    "  \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor:\n",
    "    \"\"\"\n",
    "    Class of the Random Forest Classifier Model.\n",
    "    Methods:\n",
    "        fit(X,y) -> Performs the random forests algorithm on the training set(x,y).\n",
    "        predict(x) -> Predict regression value for X.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,n_estimators,max_features =1/3,max_depth=None,criterion='mse',random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize self.\n",
    "        Inputs:\n",
    "            n_estimators: int\n",
    "                input the number of trees to grow.\n",
    "            max_depth: int\n",
    "                input the maximum depth of the tree; default is set to None.\n",
    "            criterion: string\n",
    "                input string that identifies the criterion to be used when deciding how to split each node. criterion can be: 'mse', 'friedman_mse' and 'mae' .default is set to 'mse'.\n",
    "            max_features = string or int/float:\n",
    "                input string or int/float that identifies the maximum number of features to be used when splitting each decision tree; if string can be: 'sqrt' or 'log2'; if int max_fetures will be the maximum number of features; if float the maximum number of features will be int(max_features * n_features); default is set to 1/3.             \n",
    "            random_state: int\n",
    "                input the random_state to be used on the sklearn DecisionTreeClassifier. default is set to None.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        possible_criterion = ['mse', 'friedman_mse', 'mae']\n",
    "        assert self.criterion in possible_criterion \n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        Fits the RandomForestRegressor model.\n",
    "        Inputs:\n",
    "            X: array\n",
    "                input array of input points.\n",
    "            y: array\n",
    "                input array of output points.\n",
    "        \"\"\"\n",
    "        self.input_train = X\n",
    "        self.output_train = y.reshape((-1,1))\n",
    "        self.trained_trees_list = list()\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            train_inds = np.random.choice(self.input_train.shape[0],self.input_train.shape[0],True) #generates the indices for a bootstrap sample with size N aand with repeating values(in the case of regression, there is a slight increase in error when using the technique described in the Classifier code)\n",
    "            model_tree = DecisionTreeRegressor(criterion = self.criterion, random_state = self.random_state,max_features = self.max_features,max_depth=self.max_depth)\n",
    "            model_tree = model_tree.fit(self.input_train[train_inds,:],self.output_train[train_inds,:]) #fits a decision tree with the bootstrap sample\n",
    "            self.trained_trees_list.append(model_tree)\n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Predicts the value of a given group of inputs points based on the trained trees.\n",
    "        Inputs:\n",
    "            x: array_like\n",
    "                input the input point/array to be predicted.\n",
    "        Returns:\n",
    "            final_prediction: array_like\n",
    "                outputs the value prediction of the input made by the random forest model.\n",
    "        \"\"\"\n",
    "        indiv_predictions = np.array([self.classifier.predict(x) for self.classifier in self.trained_trees_list]).T\n",
    "        final_prediction = np.zeros((indiv_predictions.shape[0],))\n",
    "        \n",
    "        for i in range(indiv_predictions.shape[0]):\n",
    "            final_prediction[i] = np.sum(indiv_predictions[i][:])/indiv_predictions.shape[1] #the final prediction of the Random Forest Regressor will be the average of all predictions made by the group of trees\n",
    "        \n",
    "        return final_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and comparing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikitlearn models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ske\n",
    "from sklearn.datasets import make_classification\n",
    "from basicMLpy.classification import acc_and_loss #calculates the accuracy of classfication tasks. basicMLpy is available at: https://github.com/HenrySilvaCS/basicMLpy\n",
    "from basicMLpy.regression import mse_and_huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests Classifier on the Wisconsin Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our model is: 97.661%\n",
      "The accuracy of the sklearn model is: 97.076%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X) # scales the data for easier computation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state=5)\n",
    "our_rfclassifier = RandomForestClassifier(n_estimators = 100, n_classes = 2)\n",
    "sk_rfclassifier = ske.RandomForestClassifier(n_estimators = 100)\n",
    "our_rfclassifier.fit(X_train,Y_train)\n",
    "sk_rfclassifier.fit(X_train,Y_train)\n",
    "our_prediction = our_rfclassifier.predict(X_test)\n",
    "sk_prediction = sk_rfclassifier.predict(X_test)\n",
    "print(f\"The accuracy of our model is: {np.round(acc_and_loss(our_prediction,Y_test)[0],3)}%\")\n",
    "print(f\"The accuracy of the sklearn model is: {np.round(acc_and_loss(sk_prediction,Y_test)[0],3)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests Regressor on the Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error of our model is: 13.466\n",
      "The Mean Squared Error of the sklearn model is: 12.743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X,y = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X) # scales the data for easier computation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state=5)\n",
    "our_rfregressor = RandomForestRegressor(n_estimators = 100, criterion = 'mse', max_features = 1/3)\n",
    "sk_rfregressor = ske.RandomForestRegressor(n_estimators = 100, criterion = 'mse', max_features = 1/3)\n",
    "our_rfregressor.fit(X_train,Y_train)\n",
    "sk_rfregressor.fit(X_train,Y_train)\n",
    "our_prediction = our_rfregressor.predict(X_test)\n",
    "sk_prediction = sk_rfregressor.predict(X_test)\n",
    "print(f\"The Mean Squared Error of our model is: {np.round(mse_and_huber(our_prediction,Y_test)[0],3)}\")\n",
    "print(f\"The Mean Squared Error of the sklearn model is: {np.round(mse_and_huber(sk_prediction,Y_test)[0],3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor on the Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error of our model is: 17.632\n",
      "The Mean Squared Error of the sklearn model is: 8.971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X,y = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X) # scales the data for easier computation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state=5)\n",
    "our_gbregressor = GBRegressor(n_estimators = 100, loss_func = 'mse', max_depth = 3)\n",
    "sk_gbregressor = ske.GradientBoostingRegressor(n_estimators = 100, criterion = 'mse', loss='ls', max_depth = 3)\n",
    "our_gbregressor.fit(X_train,Y_train)\n",
    "sk_gbregressor.fit(X_train,Y_train)\n",
    "our_prediction = our_gbregressor.predict(X_test)\n",
    "sk_prediction = sk_gbregressor.predict(X_test)\n",
    "print(f\"The Mean Squared Error of our model is: {np.round(mse_and_huber(our_prediction,Y_test)[0],3)}\")\n",
    "print(f\"The Mean Squared Error of the sklearn model is: {np.round(mse_and_huber(sk_prediction,Y_test)[0],3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Classifier on a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "def make_toy_dataset(n: int = 100, random_seed: int = None):\n",
    "    \"\"\" Generate a toy dataset for evaluating AdaBoost classifiers \"\"\"\n",
    "    \n",
    "    n_per_class = int(n/2)\n",
    "    \n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    X, y = make_gaussian_quantiles(n_samples=n, n_features=4, n_classes=2)\n",
    "    \n",
    "    return X, y*2-1\n",
    "\n",
    "X, y = make_toy_dataset(n=100, random_seed=10)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our model is: 95.0%\n",
      "The accuracy of the sklearn model is: 95.0%\n"
     ]
    }
   ],
   "source": [
    "our_abclassifier = AdaBoostClassifier(n_estimators = 100)\n",
    "sk_abclassifier = ske.AdaBoostClassifier(n_estimators = 100)\n",
    "our_abclassifier.fit(X_train,Y_train)\n",
    "sk_abclassifier.fit(X_train,Y_train)\n",
    "our_prediction = our_abclassifier.predict(X_test)\n",
    "sk_prediction = sk_abclassifier.predict(X_test)\n",
    "print(f\"The accuracy of our model is: {np.round(acc_and_loss(our_prediction,Y_test)[0],3)}%\")\n",
    "print(f\"The accuracy of the sklearn model is: {np.round(acc_and_loss(sk_prediction,Y_test)[0],3)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our models have a similar performance to the scikitlearn pre-built ones, which is a good indication that our implementations are efficient and competitive when compared to the ensemble algorithms available at other libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
